how are llms able to understand human input whether it be english or any other language or code?
-> first of all llms cannot understand human readable text (code/text). so there are layers which have thier own responsibilities to make sure
user provided text is converted into llm understandable logic

as we know every char has asci code for itself
- so one thing we can do is just convert this text into ascii and fed to llm. but this is not an ideal/working solution, as asci code for example take a simple sentence
"I build software" and conver to ascii

"I build software"
ASCII -> 
I   â   b   u   i   l   d   â   s   o   f   t   w   a   r   e
73  32  98  117 105 108 100 32  115 111 102 116 119 97  114 101

Thatâ€™s 16 numbers.

What the model must learn
That [115,111,102,116,119,97,114,101] means "software"
That [98,117,105,108,100] means "build"
That these byte sequences matter together, not individually

So the model has to learn words character by character remember long sequences discover boundaries itself This is inefficient and expensive.

Option B: Tokenization (what we actually do)

Same text: "I build software"
Tokenized as: ["I", " build", " software"]
Mapped to IDs: [40, 1723, 10456]

Now the model sees:
3 semantic units, not 16 bytes meaningful chunks already grouped

Why Tokenization and not ASCII?
ASCI has representttaion for each charcter from A-Z and a-x also 0-9 and special chars. but it does not have context for meaning, phrases and relation, which makes it
inefficient. if we add asci and fed it to llms that would have large token size as every char will have  a number so a word will have a large token which is not very 
ideal. som things such as context of words, phrases and relations in not provided in ascii and also the token size will increase as hell. cause of less efficiency is
length of tokens and context.

TOKENIZATION?
each words has token (not every char) for exmaple we have a phrase "i would not help if it would be this way" would will be replaced here with one single representttaion and 
many such repetations, so every new duplication is merged with eixsiting one and would have same number representttaion. also every phrase and words has context here. let's deep dive 

[
hola guysðŸ‘‹  
i'm aayush, I build software in JS/TS, drink coffee â˜• and ship fast ðŸš€.  
Price: â‚¹499 â€¢ Users: 1,024 â€¢ Status: OK âœ…  
Code snippet: const sum = (a, b) => a + b;
à¤¨à¤®à¤¸à¥à¤¤à¥‡ from India ðŸ‡®ðŸ‡³    
]

in Tokenization we just try to find similar chars/words and group them as pair and assgn a representttaion for me them such as if ay apperas multiple times we will
pair ay as X and use it, what this does is avoids to many chars or duplication and minimizes token size


BYTE PAIR ENDCODING??
so whatever we talked above was to term about bpe in simple terms. let's read what wiki has to say: our goal is to reduce token count so that i/p tokens and eventually
o/p tokens are reduces so as a user we can use llms for longest of period. if using less tokens per conversation cycle -> long conversations else -> shorter

so if input consists words such as aaabdaaabac  so originally bpe algorithm was used to compress text of string into smaller strings by creating pairs and assinging them single char and 
performing bpe unless no pair is been found lets psuedo this out


data -> [aaabdaaabac]

step 1 -> finding pairs which are clubbed together and assign them with X
step 2 -> aa = x
step 3 -> XabdXabac [perfomed bpe for first time]
step 4 -> Xa = Z so -> ZbdZbac [bpe for second time as we had scope for finding pair/char which was repeating and could be paired]
step 5 -> Zb = Y so -> YdYac [bpe]
step 6 -> exit bpe as no scope for pairing and assinging

FROM WIKIPEDIA -->> This data cannot be compressed further by byte-pair encoding because there are no pairs of bytes that occur more than once.To decompress the data, simply perform the replacements in the reverse order.

so we came from aaabdaaabac to YdYac [reduction of characters size will eventually reduce the size of tokens]


FLOW 

1) RAW BYTES (CHAR REPRESENTATION OF UTF-8 & ASCII FOR EXAMPLE I = 73, J = 79 AND ETC)
2) PAIRING TWO CHARS NEXT TO EACH OTHER AND COUNTING ITS OCCURING IN DATA SUCH AS 10 [73, 79] WE HAVE GOT ITERATIONS OF I AND J IN DATA AND 10 TIMES IN DATA
3) NOW A FUNCTION TO PERFROM TOKEN SWAPING OIF THESE PAIRED THING WITH RAW FINAL TOKEN (RECIEVES RAW BYTES [CHAR REPRESENTATION], STORES PAIRING OF BYTES AND APPEARENCE COUNT, NEW TOKENID)
4) TOKENIZE FUNCTION TO CALL THIS BYTES, PAIRING OPERATING, COUNTS AND ASSIGING IT TO NEW TOKEN ID
5) ENCODING AND DECODING USING THIS TOKENIZE FUNCTION

THIS IS HOW WE TOKENIZE USER PROVIDED INPUT FROM RAW TEXT TO COMPRESSED [NUMBERS] FORM FOR LLMS TO UNDERSTAND 
THE STEP OF data -> [aaabdaaabac] RAW TEXT -> CONVERT TO UTF-8 
WE IETRATE THROUGH PAIRS [NUM1, NUM2] AND OCCURING
step 1 -> finding pairs which are clubbed together and assign them with X 
step 2 -> aa = x REPLACE PAIRINGSTATE = TOKENID i.e aa = x equivalent to replacing pairedStateAndItsOccuring with new tokenId
step 3 -> XabdXabac [perfomed bpe for first time]